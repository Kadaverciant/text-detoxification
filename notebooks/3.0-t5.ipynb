{"cells":[{"cell_type":"markdown","metadata":{},"source":["# T5-base"]},{"cell_type":"markdown","metadata":{},"source":["For this model I used [this notebook](https://www.kaggle.com/code/bunnyyy/t5-tuning-for-paraphrasing-questions/notebook) as reference."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2023-11-04T23:16:33.829382Z","iopub.status.busy":"2023-11-04T23:16:33.829103Z","iopub.status.idle":"2023-11-04T23:16:36.549760Z","shell.execute_reply":"2023-11-04T23:16:36.549020Z","shell.execute_reply.started":"2023-11-04T23:16:33.829351Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\kadav\\VSCodeProjects\\PMLDL\\Assignment\\text-detoxification\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import random\n","import argparse\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","import pytorch_lightning as pl"]},{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:36.551316Z","iopub.status.busy":"2023-11-04T23:16:36.551052Z","iopub.status.idle":"2023-11-04T23:16:37.338726Z","shell.execute_reply":"2023-11-04T23:16:37.337846Z","shell.execute_reply.started":"2023-11-04T23:16:36.551290Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>non-toxic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>I'm not gonna have a child... ...with the same...</td>\n","      <td>I'm not going to breed kids with a genetic dis...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>They're all laughing at us, so we'll kick your...</td>\n","      <td>they're laughing at us. We'll show you.</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Come on, Cal, leave that shit alone.</td>\n","      <td>come on, Cal, put it down.</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Real life starts the first time you fuck, kid.</td>\n","      <td>boy, real life starts up first.</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Shit, this one I can't even pronounce.</td>\n","      <td>gosh, I can't even pronounce this.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                toxic  \\\n","5   I'm not gonna have a child... ...with the same...   \n","6   They're all laughing at us, so we'll kick your...   \n","13               Come on, Cal, leave that shit alone.   \n","22     Real life starts the first time you fuck, kid.   \n","25             Shit, this one I can't even pronounce.   \n","\n","                                            non-toxic  \n","5   I'm not going to breed kids with a genetic dis...  \n","6             they're laughing at us. We'll show you.  \n","13                         come on, Cal, put it down.  \n","22                    boy, real life starts up first.  \n","25                 gosh, I can't even pronounce this.  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"../data/raw/suitable.csv\", index_col=0)\n","df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.340256Z","iopub.status.busy":"2023-11-04T23:16:37.339954Z","iopub.status.idle":"2023-11-04T23:16:37.346161Z","shell.execute_reply":"2023-11-04T23:16:37.345188Z","shell.execute_reply.started":"2023-11-04T23:16:37.340227Z"},"trusted":true},"outputs":[{"data":{"text/plain":["349705"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["len(df)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.347918Z","iopub.status.busy":"2023-11-04T23:16:37.347494Z","iopub.status.idle":"2023-11-04T23:16:37.426332Z","shell.execute_reply":"2023-11-04T23:16:37.425229Z","shell.execute_reply.started":"2023-11-04T23:16:37.347885Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10000 1000\n"]}],"source":["seed = 177013\n","train = df.sample(10000, random_state=seed)\n","val = df.drop(train.index).sample(1000, random_state=seed)\n","print(len(train), len(val))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.454089Z","iopub.status.busy":"2023-11-04T23:16:37.453798Z","iopub.status.idle":"2023-11-04T23:16:37.494039Z","shell.execute_reply":"2023-11-04T23:16:37.493163Z","shell.execute_reply.started":"2023-11-04T23:16:37.454062Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(177013)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.495525Z","iopub.status.busy":"2023-11-04T23:16:37.495242Z","iopub.status.idle":"2023-11-04T23:16:37.529212Z","shell.execute_reply":"2023-11-04T23:16:37.528183Z","shell.execute_reply.started":"2023-11-04T23:16:37.495497Z"},"trusted":true},"outputs":[],"source":["class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparams):\n","        super(T5FineTuner, self).__init__()\n","        self.hparams = hparams\n","\n","        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","\n","    def is_logger(self):\n","        return False\n","\n","    def forward(\n","            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n","    ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            lm_labels=lm_labels,\n","        )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            lm_labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","        return {\"val_loss\": loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n","        if self.trainer.use_tpu:\n","            xm.optimizer_step(optimizer)\n","        else:\n","            optimizer.step()\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","\n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","\n","    def train_dataloader(self):\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n","                                num_workers=4)\n","        t_total = (\n","                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","                // self.hparams.gradient_accumulation_steps\n","                * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n","        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.556210Z","iopub.status.busy":"2023-11-04T23:16:37.555775Z","iopub.status.idle":"2023-11-04T23:16:37.574463Z","shell.execute_reply":"2023-11-04T23:16:37.573550Z","shell.execute_reply.started":"2023-11-04T23:16:37.556167Z"},"trusted":true},"outputs":[],"source":["class GrammerDataset(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n","        self.path = os.path.join(data_dir, type_path + '.csv')\n","\n","        self.question = \"toxic\"\n","        self.target_column = \"non-toxic\"\n","        self.data = pd.read_csv(self.path)\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n","\n","        self._build()\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","        target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","\n","    def _build(self):\n","        for idx in range(len(self.data)):\n","            target,question= self.data.loc[idx, self.target_column], self.data.loc[idx, self.question]\n","            input_ = \"detoxify: \"+ question + ' </s>'\n","            target = target + \" </s>\"\n","\n","            # tokenize inputs\n","            tokenized_inputs = self.tokenizer.batch_encode_plus(\n","                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n","            )\n","            # tokenize targets\n","            tokenized_targets = self.tokenizer.batch_encode_plus(\n","                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n","            )\n","\n","            self.inputs.append(tokenized_inputs)\n","            self.targets.append(tokenized_targets)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:39.035014Z","iopub.status.busy":"2023-11-04T23:16:39.034683Z","iopub.status.idle":"2023-11-04T23:16:39.044932Z","shell.execute_reply":"2023-11-04T23:16:39.044209Z","shell.execute_reply.started":"2023-11-04T23:16:39.034986Z"},"trusted":true},"outputs":[],"source":["def get_dataset(tokenizer, type_path, args):\n","    return GrammerDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.575940Z","iopub.status.busy":"2023-11-04T23:16:37.575544Z","iopub.status.idle":"2023-11-04T23:16:37.705630Z","shell.execute_reply":"2023-11-04T23:16:37.704864Z","shell.execute_reply.started":"2023-11-04T23:16:37.575909Z"},"trusted":true},"outputs":[],"source":["train.to_csv('../data/raw/train.csv', index=False)\n","val.to_csv('../data/raw/valid.csv', index= False)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.707168Z","iopub.status.busy":"2023-11-04T23:16:37.706860Z","iopub.status.idle":"2023-11-04T23:16:38.058180Z","shell.execute_reply":"2023-11-04T23:16:38.057169Z","shell.execute_reply.started":"2023-11-04T23:16:37.707128Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 2.73MB/s]\n","c:\\Users\\kadav\\VSCodeProjects\\PMLDL\\Assignment\\text-detoxification\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kadav\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 3.73MB/s]\n","Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<?, ?B/s]\n","c:\\Users\\kadav\\VSCodeProjects\\PMLDL\\Assignment\\text-detoxification\\.venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = T5Tokenizer.from_pretrained('t5-base')"]},{"cell_type":"markdown","metadata":{},"source":["## Model training"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:37.544304Z","iopub.status.busy":"2023-11-04T23:16:37.543901Z","iopub.status.idle":"2023-11-04T23:16:37.554370Z","shell.execute_reply":"2023-11-04T23:16:37.553309Z","shell.execute_reply.started":"2023-11-04T23:16:37.544263Z"},"trusted":true},"outputs":[],"source":["args_dict = dict(\n","    data_dir='../data/raw', # path for data files\n","    output_dir='../models/t5', # path to save the checkpoints\n","    model_name_or_path='t5-base',\n","    tokenizer_name_or_path='t5-base',\n","    max_seq_length=64,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=32,\n","    eval_batch_size=32,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=32,\n","    n_gpu=1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    save_strategy=\"no\",\n","    seed=177013,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:39.012867Z","iopub.status.busy":"2023-11-04T23:16:39.012572Z","iopub.status.idle":"2023-11-04T23:16:39.017961Z","shell.execute_reply":"2023-11-04T23:16:39.017095Z","shell.execute_reply.started":"2023-11-04T23:16:39.012837Z"},"trusted":true},"outputs":[],"source":["args = argparse.Namespace(**args_dict)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:39.019449Z","iopub.status.busy":"2023-11-04T23:16:39.019058Z","iopub.status.idle":"2023-11-04T23:16:39.033475Z","shell.execute_reply":"2023-11-04T23:16:39.032712Z","shell.execute_reply.started":"2023-11-04T23:16:39.019407Z"},"trusted":true},"outputs":[],"source":["train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n","    early_stop_callback=False,\n","    precision= 16 if args.fp_16 else 32,\n","    amp_level=args.opt_level,\n","    gradient_clip_val=args.max_grad_norm,\n","    checkpoint_callback=False,\n","    logger=False,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:39.046399Z","iopub.status.busy":"2023-11-04T23:16:39.046057Z","iopub.status.idle":"2023-11-04T23:16:46.532414Z","shell.execute_reply":"2023-11-04T23:16:46.531438Z","shell.execute_reply.started":"2023-11-04T23:16:39.046371Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialize model\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","CUDA_VISIBLE_DEVICES: [0]\n"]}],"source":["print (\"Initialize model\")\n","model = T5FineTuner(args)\n","\n","trainer = pl.Trainer(**train_params)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:16:46.534246Z","iopub.status.busy":"2023-11-04T23:16:46.533853Z","iopub.status.idle":"2023-11-04T23:22:20.648800Z","shell.execute_reply":"2023-11-04T23:22:20.647729Z","shell.execute_reply.started":"2023-11-04T23:16:46.534206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" Training model\n"]},{"name":"stderr","output_type":"stream","text":["\n","  | Name  | Type                       | Params\n","-----------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 222 M \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae5438aa475947fdb2296be5c8618eb0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","training finished\n","Saving model\n","Saved model\n"]}],"source":["print (\" Training model\")\n","trainer.fit(model)\n","\n","print (\"training finished\")\n","\n","print (\"Saving model\")\n","model.model.save_pretrained(\"../models/t5\")\n","\n","print (\"Saved model\")"]},{"cell_type":"markdown","metadata":{},"source":["## Check model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:22:20.651313Z","iopub.status.busy":"2023-11-04T23:22:20.650666Z","iopub.status.idle":"2023-11-04T23:22:27.271448Z","shell.execute_reply":"2023-11-04T23:22:27.270700Z","shell.execute_reply.started":"2023-11-04T23:22:20.651261Z"},"trusted":true},"outputs":[],"source":["my_model = T5ForConditionalGeneration.from_pretrained(\"../models/t5\")\n","my_model.eval()\n","my_model.config.use_cache = False"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:22:27.273251Z","iopub.status.busy":"2023-11-04T23:22:27.272859Z","iopub.status.idle":"2023-11-04T23:22:27.659224Z","shell.execute_reply":"2023-11-04T23:22:27.657920Z","shell.execute_reply.started":"2023-11-04T23:22:27.273211Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["my_tokenizer = T5Tokenizer.from_pretrained('t5-base')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T23:22:27.661173Z","iopub.status.busy":"2023-11-04T23:22:27.660765Z","iopub.status.idle":"2023-11-04T23:22:37.400595Z","shell.execute_reply":"2023-11-04T23:22:37.399781Z","shell.execute_reply.started":"2023-11-04T23:22:27.661131Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'last time I was drunk, I sucked firemen.'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["def generate(model, tokenizer, prompt):\n","    input_ = \"detoxify: \"+ prompt + ' </s>'\n","    input_ids = tokenizer.batch_encode_plus(\n","                [input_], max_length=64, truncation=True, return_tensors=\"pt\"\n","            ).input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","generate(my_model, my_tokenizer, \"last time I got drunk, I fucked the firemen.\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000\n"]}],"source":["test = df.drop(train.index).drop(val.index).sample(1000, random_state=seed)\n","print(len(test))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>non-toxic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>399522</th>\n","      <td>I'll show you who's got bigger butts on this s...</td>\n","      <td>I'll show you who's got The bigger bottom roun...</td>\n","    </tr>\n","    <tr>\n","      <th>32832</th>\n","      <td>\"I sit on the floor and pick my nose and think...</td>\n","      <td>\"I sit down and I pick my nose, and on my nose...</td>\n","    </tr>\n","    <tr>\n","      <th>126205</th>\n","      <td>did you know I really thought this grease monk...</td>\n","      <td>You know, I thought that greaser really did tu...</td>\n","    </tr>\n","    <tr>\n","      <th>151854</th>\n","      <td>Just go, tyler, get the hell away from me.</td>\n","      <td>go, Tyler, get away from me.</td>\n","    </tr>\n","    <tr>\n","      <th>218096</th>\n","      <td>'You're putrid, you know that?' Danny said.</td>\n","      <td>\"you're cute, you know?\" Said Danny.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                    toxic  \\\n","399522  I'll show you who's got bigger butts on this s...   \n","32832   \"I sit on the floor and pick my nose and think...   \n","126205  did you know I really thought this grease monk...   \n","151854         Just go, tyler, get the hell away from me.   \n","218096        'You're putrid, you know that?' Danny said.   \n","\n","                                                non-toxic  \n","399522  I'll show you who's got The bigger bottom roun...  \n","32832   \"I sit down and I pick my nose, and on my nose...  \n","126205  You know, I thought that greaser really did tu...  \n","151854                       go, Tyler, get away from me.  \n","218096               \"you're cute, you know?\" Said Danny.  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\kadav\\VSCodeProjects\\PMLDL\\Assignment\\text-detoxification\\.venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n","  warnings.warn(\n","c:\\Users\\kadav\\VSCodeProjects\\PMLDL\\Assignment\\text-detoxification\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>non-toxic</th>\n","      <th>generated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>399522</th>\n","      <td>I'll show you who's got bigger butts on this s...</td>\n","      <td>I'll show you who's got The bigger bottom roun...</td>\n","      <td>I'll show you who's bigger on this station!</td>\n","    </tr>\n","    <tr>\n","      <th>32832</th>\n","      <td>\"I sit on the floor and pick my nose and think...</td>\n","      <td>\"I sit down and I pick my nose, and on my nose...</td>\n","      <td>\"I sit on the floor and pick my nose and think...</td>\n","    </tr>\n","    <tr>\n","      <th>126205</th>\n","      <td>did you know I really thought this grease monk...</td>\n","      <td>You know, I thought that greaser really did tu...</td>\n","      <td>did you know I thought this grease monkey was ...</td>\n","    </tr>\n","    <tr>\n","      <th>151854</th>\n","      <td>Just go, tyler, get the hell away from me.</td>\n","      <td>go, Tyler, get away from me.</td>\n","      <td>just go, tyler, get away from me.</td>\n","    </tr>\n","    <tr>\n","      <th>218096</th>\n","      <td>'You're putrid, you know that?' Danny said.</td>\n","      <td>\"you're cute, you know?\" Said Danny.</td>\n","      <td>\"You know what?\" Danny asked.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                    toxic  \\\n","399522  I'll show you who's got bigger butts on this s...   \n","32832   \"I sit on the floor and pick my nose and think...   \n","126205  did you know I really thought this grease monk...   \n","151854         Just go, tyler, get the hell away from me.   \n","218096        'You're putrid, you know that?' Danny said.   \n","\n","                                                non-toxic  \\\n","399522  I'll show you who's got The bigger bottom roun...   \n","32832   \"I sit down and I pick my nose, and on my nose...   \n","126205  You know, I thought that greaser really did tu...   \n","151854                       go, Tyler, get away from me.   \n","218096               \"you're cute, you know?\" Said Danny.   \n","\n","                                                generated  \n","399522        I'll show you who's bigger on this station!  \n","32832   \"I sit on the floor and pick my nose and think...  \n","126205  did you know I thought this grease monkey was ...  \n","151854                  just go, tyler, get away from me.  \n","218096                      \"You know what?\" Danny asked.  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["test['generated'] = test['toxic'].map(lambda prompt: generate(my_model, my_tokenizer, prompt))\n","test.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Save results"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["test.to_csv(\"../data/interim/t5_pred.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
